class: ProximalBertPruningManager
sparsity_config: bert_base_sparsity_config.example.yaml
init_weights_path: nvidia-bert-base-uncased-pytorch_model.bin
admm_resume_from_checkpoint: false

initial_rho: 0.001
rho_num: 1
initial_lambda: 0.0001
cross_x: 1
cross_f: 1
update_freq: 300 # 30
lr: 0.006
admm_steps: 3000  # 300
retrain_steps: 3000 # 300

sparsity_type: threshold
arch: bert_base_uncased
optimizer_type: NVLAMB
dataset_type: EnwikiBookcorpus
save_dir: test_pretrain_admm_rho1e-3_lambda1e-4_1x1_lr6e-3_full_fledge
output_dir: /home/CORP.PKUSC.ORG/hatsu3/research/lab_projects/bert/notebooks/nvidia_bert/results/checkpoints_rho1e-3_lambda1e-4_1x1_lr6e-3_full_fledge
overwrite: true
admm_ckpt_steps: 600 # 200
retrain_ckpt_steps: 600 # 200

fp16: true
tensorboard_logdir: tb_logs_rho1e-3_lambda1e-4_1x1_lr6e-3_full_fledge
tensorboard_json_path: tb_logs_rho1e-3_lambda1e-4_1x1_lr6e-3_full_fledge/all_scalars.json

accumulate_into_fp16: false
allreduce_post_accumulation: true
allreduce_post_accumulation_fp16: true
bert_model: bert-base-uncased
checkpoint_activations: false
config_file: data/download/google_pretrained_weights/uncased_L-12_H-768_A-12/bert_config.json
do_train: true
gradient_accumulation_steps: 64  # 512
input_dir: data/hdf5_lower_case_1_seq_len_512_max_pred_80_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/books_wiki_en_corpus/
learning_rate: 0.004
local_rank: -1
log_freq: 1.0
loss_scale: 0.0  # original: 0.0 (dynamic scaling)
max_predictions_per_seq: 80
max_seq_length: 512
max_steps: 1563.0
num_steps_per_checkpoint: 200
num_train_epochs: 3.0
phase1_end_step: 7038
phase2: true
resume_from_checkpoint: false
resume_step: -1
seed: 42
train_batch_size: 512 # 4096
warmup_proportion: 0.128
