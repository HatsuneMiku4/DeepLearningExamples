accumulate_into_fp16: false
allreduce_post_accumulation: true
allreduce_post_accumulation_fp16: true
bert_model: bert-large-uncased
checkpoint_activations: false
config_file: data/download/google_pretrained_weights/uncased_L-12_H-768_A-12/bert_config.json
do_train: true
fp16: false
gradient_accumulation_steps: 128
input_dir: data/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/books_wiki_en_corpus/
learning_rate: 0.006
local_rank: -1
log_freq: 1.0
loss_scale: 0.0
max_predictions_per_seq: 20
max_seq_length: 128
max_steps: 7038.0
num_steps_per_checkpoint: 200
num_train_epochs: 3.0
output_dir: /home/CORP.PKUSC.ORG/hatsu3/research/lab_projects/bert/notebooks/nvidia_bert/results/checkpoints
phase1_end_step: 7038
phase2: false
resume_from_checkpoint: false
resume_step: -1
seed: 42
train_batch_size: 8192
warmup_proportion: 0.2843
admm_config: bert_base_pretrain_proximal_admm_config.example.yaml